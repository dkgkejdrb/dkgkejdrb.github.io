---
layout: single
title:  "논문 리뷰(2): Training language models to follow instructions with human feedback."
categories: 석사_컴퓨터공학
tag: [석사_컴퓨터공학]
toc: true















---

## 1. 논문 정보

- 제목: Training language models to follow instructions with human feedback
- 저자: *Long Ouyang* et al
- doi: **[ arXiv:2203.02155](https://arxiv.org/abs/2203.02155)**

----

## 2. 요약

 언어 모델의 크기를 늘린다고 해서 사용자 친화적 모델이 되는 것은 아니다. 예를 들어, 대규모 언어 모델(LLM)은 거짓이나 유해 피드백, 또는 사용자에게 도움이 되지 않는 피드백을 생성할 수 있다. 이 논문에서는 인간 피드백을 활용한 파인 튜닝을 통해 언어 모델을 다양한 작업에 대해 사용자 의도와 일치시키는 방법, 즉 InstructGPT를 제안한다.

 먼저 스크린 테스트를 통해 선발된 40명의 라벨러가 Supervised Fine Tuning(SFT)에 필요한 데모 데이터를 생성하거나 모델이 생성한 응답을 평가하는 역할을 맡는다. 이후 GPT-3를 다음과 같은 세 단계로 훈련한다.



  **단계** **1. Supervised Fine Tuning(SFT)**

![그림입니다. 원본 그림의 이름: CLP00002dc40001.bmp 원본 그림의 크기: 가로 357pixel, 세로 614pixel](../../images/2024-06-22-a2/EMB000009d838b3.bmp)  

 이 단계에서는 GPT API로 제출된 프롬프트를 수집하고, 라벨러가 원하는 출력 행동을 보여주는 데모 데이터를 직접 작성한다. 이렇게 수집된 데이터를 이용해 GPT-3를 지도 학습 방식으로 훈련한다.

 SFT 모델은 총 16 epoch 동안 훈련되었으며 1 epoch 후에 validation loss(모델 수행측정 지표)에서 오버핏 경향을 보였다.



**단계** **2. Reward Model(RM)**

  ![그림입니다. 원본 그림의 이름: CLP00002dc40002.bmp 원본 그림의 크기: 가로 357pixel, 세로 671pixel](../../images/2024-06-22-a2/EMB000009d838b8.bmp)  

 SFT 단계에서 훈련된 모델에 어떠한 프롬프트를 제공하고, 모델이 생성한 다양한 응답에 대해 라벨러가 순위를 매긴다(best~worst). 이 데이터를 활용해 보상 모델인 RM을 훈련한다.

 이 논문에서는 계산 자원을 절약하기 위해 6B 크기의 보상 모델을 사용했다. 이는 175B 크기의 보상 모델 훈련이 불안정할 수 있으며, 강화 학습 동안 보상 함수로 사용하기에 부적합하다는 것을 발견한 결과이다.



**단계** **3. Proximal Policy Optimization (PPO)**

  ![그림입니다. 원본 그림의 이름: CLP00002dc40003.bmp 원본 그림의 크기: 가로 395pixel, 세로 706pixel](../../images/2024-06-22-a2/EMB000009d838bc.bmp)  

 마지막으로 RM을 보상 함수로 적용하고 PPO 알고리즘을 사용해 SFT로 훈련된 GPT-3를 지속적으로 파인 튜닝한다. 

 환경은 무작위 사용자 프롬프트를 SFT 모델에 제시하고 응답 결과를 RM에 전달하여 보상을 생성하고 에피소드를 종료한다. 또한, 보상 모델의 과도한 최적화를 해소하기 위해 각 토큰에서 SFT 모델의 토큰별 KL 패널티를 추가한다.

 또한, 공공 NLP(Natural Language Processing) 데이터셋에서의 성능 저하를 해결하기 위해 사전 훈련 그라디언트를 PPO 그라이언트와 혼합하는 PPO-ptx 모델을 개발하였다.

 다음으로, 모델들의 성능 평가는 두 종류의 정량적 평가를 통해 실시되었다. 첫 번째는 API distribution에 대한 평가로, API를 통해 수집된 프롬프트 중에서 훈련에 사용되지 않은 사용자의 프롬프트를 선택하여 각 모델로부터 응답을 수집한다. 이때 17B SFT 모델을 기준으로, 모델들의 전반적인 품질을 Likert 척도를 사용하여 라벨러가 평가하도록 하였으며, 각 모델의 응답에 대한 다양한 메타데이터도 수집되었다. 두 번째는 공공 NLP 데이터셋에 대한 평가로, 이는 언어 모델의 진실성, 유해성, 편향을 평가하는 데이터셋과 질문 응답, 독해, 요약과 같은 전통적인 NLP 작업에 대한 제로샷 성능을 포함한다.

---

## 3. 결론

  ![그림입니다. 원본 그림의 이름: CLP00000b5c0da9.bmp 원본 그림의 크기: 가로 716pixel, 세로 409pixel](../../images/2024-06-22-a2/EMB000009d838c0.bmp)  

 라벨러들은 GPT-3의 응답보다 InstructGPT의 응답을 현저하게 선호했으며 GPT-3의 응답부터 PPO-ptx까지 단계적 개선이 이루어졌음을 알 수 있다.

---

## 4. 느낀점

 본 논문 연구를 통해, 사용자 친화적 LLM을 개발하기 위해선 적절한 인간 생성 데모 데이터가 매우 중요함을 인지하게 되었습니다. 그러나 논문의 저자와 같이 라벨러를 고용할 여유가 없어, 제가 가진 데이터셋 내에서 초등학생 학습자의 연구 문서에 대해 선생님이 제공한 피드백을 활용하여 GPT-3.5를 파인 튜닝하는 방법을 고려하고 있습니다.

 논문의 아이디어 단계에서, 초등학생의 연구 문서 데이터베이스에 접근하여, 학생의 연구 주제와 연구 문서, 그리고 그에 대한 선생님의 피드백을 하나의 세트로 묶을 계획입니다. 이 세트마다 선생님의 피드백의 유용성을 Likert 척도로 평가하여, 높은 평가를 받은 피드백을 포함하는 세트를 선별하여 GPT-3.5 파인 튜닝에 사용할 예정입니다. 이어서 SFT 모델과 GPT-3.5 모델이 두가지 유형의 질문(1. 제로샷 성능 측정 질문, 2. SFT에 사용한 질문)에 대해 어떻게 응답하는지를 실험하고 수집합니다. 이를 Likert 척도를 사용해 인간 평가할 계획입니다. 이 과정에서는 외부 실험 인력을 선발하거나 고용할 필요가 있을 것으로 보입니다.

 특히, 초등학생 학습자의 연구 문서에 대한 SFT 모델의 피드백은 유용하고 정직하며 무해해야 하기 때문에, 미세 조정 단계에서 양질의 인간 생성 데모 데이터셋 확보의 중요성을 깨닫게 되었습니다.