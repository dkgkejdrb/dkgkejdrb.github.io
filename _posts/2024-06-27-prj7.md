---
layout: single
title:  "석사 학위 논문: A GPT-based Code Review System for Programming Language Learning"
categories: 석사_컴퓨터공학
tag: [석사_컴퓨터공학]
toc: true















---

# 논문 정보

- 제목(영어): A  GPT-based Code Review System for Programming Language Learning 
- 제목(국문): 프로그래밍 언어 학습 지원을 위한 GPT 기반 코드 리뷰 시스템
- 저자: 이동규(본인)
- 종류: 컴퓨터공학 석사 학위 논문
- arxiv: 업로드 완료 후 심사 대기중
- 키워드: Large Language Models (LLMs), GPT-4, Programming Language Education, Learner-Friendly Code Reviews

---

# **A  GPT-based Code Review System for Programming Language Learning**

 <br />

**Lee Dong-Kyu**

Department of Electrical Engineering and Computer Science, 

University of Hanyang Seoul, 

Republic of Korea

 <br />

---

# Abstract

The increasing demand for programming language education and growing class sizes require immediate and personalized feedback. However, traditional code review methods have limitations in providing this level of feedback. As the capabilities of Large Language Models (LLMs) like GPT for generating accurate solutions and timely code reviews are verified, this research proposes a system that employs GPT-4 to offer learner-friendly code reviews and minimize the risk of AI-assist cheating.

 <br />

To provide learner-friendly code reviews, a dataset was collected from an online judge system, and this dataset was utilized to develop and enhance the system’s prompts. In addition, to minimize AI-assist cheating, the system flow was designed to provide code reviews only for code submitted by a learner, and a feature that highlights code lines to fix was added. After the initial system was deployed on the web, software education experts conducted usability test. Based on the results, improvement strategies were developed to improve code review and code correctness check module, thereby enhancing the system. 

  <br />

The improved system underwent evaluation by software education experts based on four criteria: strict code correctness checks, response time, lower API call costs, and the quality of code reviews. The results demonstrated a performance to accurately identify error types, shorten response times, lower API call costs, and maintain high-quality code reviews without major issues. Feedback from participants affirmed the tool’s suitability for teaching programming to primary and secondary school students. Given these benefits, the system is anticipated to be a efficient learning tool in programming language learning for educational settings.



# 국문요지

프로그래밍 언어 교육에 대한 수요가 증가하고 수업 규모가 커지면서 즉각적이고 개별적인 피드백이 필요하다. 그러나 기존의 코드 리뷰 방법은 이러한 수준의 피드백을 제공하는 데 한계가 있다. GPT와 같은 대규모 언어 모델(LLMs)이 정확한 해결책을 생성하고 시기적절한 코드 리뷰를 제공하는 능력이 검증됨에 따라, 본 연구는 GPT-4를 활용하여 학습자 친화적인 코드 리뷰를 제공하고 AI 보조 부정행위의 위험을 최소화하는 시스템을 제안한다.

 <br />

본 시스템은 학습자 친화적인 코드 리뷰를 제공하기 위해, 온라인 저지 시스템에서 데이터셋을 수집하여 프롬프트를 개발하고 개선하였다. 또한, AI 보조 부정행위를 최소화하기 위해, 학습자가 제출한 코드에만 리뷰를 제공하도록 시스템 플로우를 설계하였고, 수정이 필요한 코드 줄을 강조하여 보여주는 편의 기능도 추가되었다. 초기 시스템은 웹에 배포된 후 소프트웨어 교육 전문가들이 사용성 테스트를 실시하였으며, 이 결과를 바탕으로 코드 리뷰와 코드 정확성 검사 기능을 개선하는 전략을 수립하여 시스템을 개선하였다.

 <br />

개선된 시스템은 이후 소프트웨어 교육 전문가들에 의해 엄격한 코드 정확성 검사, 응답 시간, 낮은 API 호출 비용, 코드 리뷰의 질을 기준으로 평가되었다. 결과는 오류 유형을 정확히 식별하고 응답 시간을 단축하며 API 비용을 낮추고 주요 문제 없이 고품질의 코드 리뷰를 유지하는 성능을 보여주었다. 참가자들은 일관되게 이 도구가 초등학교와 중등학교 학생들에게 프로그래밍 언어를 가르치기에 적합하다고 응답했다. 이러한 이점을 고려할 때, 이 시스템은 교육 환경에서 프로그래밍 언어 학습을 위한 효율적인 학습 도구가 될 것으로 기대된다.

---

# **Chapter1. Introduction**

## **1.1 Background**

Digital Transformation across all sectors of society is currently having a positive impact on various socio-economic areas. At this point of Digital Transformation, a global trend is forming that emphasizes the importance of education to equip everyone with “Digital Competencies” so they can participate in and benefit evenly from the digital world [1]. According to the Organization for Economic Cooperation and Development Skill Outlook 2019 report [2], Digital competencies will be closely related to all jobs in the future, so that there is a need to cultivate comprehensive digital skills throughout life.

 <br />

In response to this global trend, Educational institutions in Korea have also enhanced education in order to foster digital and artificial intelligence(AI) literacy through National Curriculum of 2022 revision from Ministry of Education. Particularly in primary and secondary schools, Computer and Information education is being expanded by increasing either Computer and Information class or discretionary time [3]. While Computer and Information education has become mandatory in primary and secondary schools, Educational institutions can allow the introduction of various coding courses tailored to students’ career paths and aptitudes. Primary schools can now organize and operate Practical Science for previously 17 hours, but now over 34 hours. Secondary schools can organize and operate Computer and Information subject for previously 34 hours, but now over 68 hours. In addition, most core subjects such as Korean language, Science, Social Studies, and Art are increasingly related with contents that promote Digital competencies.

 <br />

## **1.2 Challenge**

To strengthen digital competences in primary and secondary schools, learners have many opportunities to access online and offline programming education environments. Accordingly, the demand for programming language education is increasing, and the size of classes that must be taught in one class is also increasing. However, due to the nature of programming language education, various solutions are possible during learning. Since each learner has a different implementation method and a variety of code written, providing personalized code reviews can be an effective approach.

 <br />

However, traditional code review methods have limitations in providing immediate and personalized feedback [4], [5]. Automated test case method is provided through online code correctness check systems, but it only point out issues such as compilation errors or inconsistencies in input/output examples, which can be difficult for beginners to understand. Peer review methos conducted through online forums requires significant time and effort to provide appropriate solutions and can be challenging due to varying levels of learners [6].

 <br />

The tutor review is a method in which learners mostly rely on the instructor to receive code reviews. Although it is an effective way to provide personalized and customized code reviews, it is very difficult for instructors to provide individual support due to the nature of the offline environment where many students must be managed.

<br />

## **1.3 GPT in Programming Language Learning**

The recent emergence of GPT, a Generative Pretrained Transformer model among large language models (LLM), provides an effective solution that overcomes the limitations of traditional code review methods [6]. In addition, Codex, a language model that analyzes the natural language description requested by the user as a prompt and converts it into code, is installed in GPT and is used as a powerful programming support tool. These powerful features of GPT show that GPT has good performance not only in text generation but also in code analysis and generation, becoming an appropriate approach for immediately performing code reviews and providing personalized comments in a programming language learning support system.

<br />

However, as the performance of LLM is upgraded and its use spreads in industrial section, many concerns are raised about using LLM in an educational environment due to the characteristic of LLM that provides immediate answers to any type of problem. In particular, there are concerns about “AI-assist cheating” side effects that may occur when students directly use ChatGPT, a service that allows easy use of GPT in introductory programming courses. Therefore, for effective real-time support solutions for programming language courses, it is necessary to construct a system that can perform immediate and effective code reviews while minimizing AI-assist cheating.

<br />

Therefore, to develop pedagogically effective, real-time support solutions for programming language courses, it is crucial to create systems that can provide immediate and effective code reviews while minimizing the risks associated with AI-assist cheating. This approach will ensure that learners not only receive the support they need but also engage in their learning processes authentically and responsibly.

<br />

## **1.4 Research Purpose**

In this paper, I present a code review system using GPT-4 that supports programming language learning for primary and secondary school students. This system aims to minimize AI-assist cheating and provide learner-friendly feedback appropriate for the target age. To minimize AI-assist cheating, code review comments are provided only for code submitted by learners, and code solutions are not provided directly in comments. 

<br />

To provide learner-friendly feedback, prompts were designed considering the precision, usefulness, specificity, supportive tone, and learning effect of code review comments. The system provides students with a bank of programming exercises. When a student selects a problem to solve and submits code, the system shows the correctness of the code and a code review comment on the students’ submitted code. It is an integrated programming language learning support environment.

<br />

This system was developed considering deployment in an introductory Python Programming course for elementary, middle, and high school students. A total of two system evaluations were conducted to evaluate the system, and current instructors with more than 2 years of programming education experience were selected as participants. 

<br />

Based on the results of the first system evaluation, the system advancement strategy for this study was summarized and it was improved.

<br />

In order to effectively test the performance of the improved system’s code correctness check and code review comment, it was evaluated according to the following research queries.

l **RQ1 – Strict Code Correctness Check in Improved System:** Are the results of the GPT-based code correctness check more strict than traditional online judge system?

l **RQ2 – Response Time Reduced in Code Reviews:** Does the improved system reduce the response time for code review comments compared to the existing system?

l **RQ3 – Cost Reduction via API Call Optimization:** Does the improved system reduce API call costs compared to existing system?

l **RQ4 – Quality of Code Reviews:** Despite reduced response times and API call costs, does the system maintain the quality of code reviews?

<br />

# **Chapter2. Related Work**

## **2.1 LLMs in Educational Feedback**

As LLMs become widely used in practical applications, educational experts are exploring the potential of using GPT to generate educational feedback on student assignments. Wei Dai et al. [7] collected business scenario-related data science project proposals from students at an Australian university. The proposals were required to include a project description and business model. Instructors evaluated the submissions based on five criteria: objectives, topic relevance, business benefit, novelty, and clarity, providing text feedback subsequently. After anonymizing the students’ personal information, feedback related to 103 students proposals was compiled.

<br />

To generate text feedback on these proposals using ChatGPT, the prompt was designed with properties: project’s objectives, relevance to the data science topic, business benefit, novelty/creativity, overall clarity. Each student’s proposal text was added after the prompt and submitted to ChatGPT, which generated the feedback. Interestingly, it was noted that the instructors’ feedback was on average 109 characters less than that provided by ChatGPT. While a more word count does not necessarily equate to more effective feedback, three experts rated each piece of feedback for readability and consistency on a scale from 0 to 4, finding that ChatGPT’s feedback was more detailed and readable than that of the instructors. 

<br />

GPT could potentially replace some of the instructor’s workload while providing useful text feedback to students in educational settings.

<br />

## **2.2 Code Review Automation on Basic Programming training**

Learning tools that provide automated code feedback, such as the online judge service, deal with code submitted by students in a test case manner. This method runs the students’ programs against test cases and then provides feedback on failed cases. The error message is in English and consists of terms related to compilation errors, making it difficult for beginners to understand the cause of the issue.

 <br />

To provide automatic feedback to novice learners, Rishabh Singh [8] proposed a new method in which learning tools require error information from the instructor, including solutions and potential corrections for errors students may make to the method. Based on this information, the system can automatically identify the minimal corrections to a student’s incorrect code and provide feedback on what went wrong, along with a quantifiable measure of how wrong a submitted code is.

<br />

The feedback provided should consist of the following four pieces of information: the location of the error as a line number, the problematic expression on that line, the sub-expression that needs to be modified, and the newly modified value of the sub-expression. To provide this level of feedback, an automated feedback system requires correction rules from the instructor that indicate not only the problems students need to solve and solution, but also ways to correct the types of error students may make. The learning tool then applies these rules to the student program, searches for candidate programs that match the correct answer code and requires minimal modification, and provides feedback.

<br />

The composition of the feedback provided to novice learners and the information required from the instructor to generate the feedback are factors that must be sufficiently addressed when designing prompts for an LLM-based code review system.

<br />

## **2.3 Prompt Template for Code Review Automation**

Compared to pre-trained language models (PLMs), LLMs in code review automation provide a more efficient and general-purpose approach to complex task such as code review automation. Because LLM already internalizes various domain knowledge, it requires fewer resources for domain-specific prior training and can be flexibly applied to a variety of tasks, contributing greatly to the development of code review automation.

<br />

To automate code reviews, J. Lu et al. [9] proposed a pipeline as a cycle of the code review process that predicts the need for reviews, generates review comments, and sequentially performs code improvement tasks.

 <br />

The three core tasks essential to the code review process are:

l **Review Necessity Prediction (RNP):** A task that responds with a binary label (yes/no) whether a diff hunk requires a review

l **Review Comment Generation (RCG):** Generates a comment for a given diff hunk. Two perspectives are presented here: a line-level perspective, which focuses on the content of individual lines of code (using Crer dataset), and a method-level perspective, which provides a holistic view of the code context (using Tufano dataset)

l **Code Refinement (CR):** Code refinement involves minor adjustments or rearrangements to existing code to improve the quality of the code. Due to these minor modifications, the input code and output code often show strong similarities

The template containing the prompt request and response utilized Stanford Alpaca’s template and following the format: instructions, input(optional), output by clearly distinguishing and modularizing each step is a systematic approach. 

<br />

In terms of systematically managing prompt templates in this way, the maintenance task become efficient and effective by fixing the response results of LLM and updating the prompts in specific areas.

<br />

![fig1](../../images/2024-06-27-prj7/fig1.png)

Figure 1 The Cycle of Code Review Process 

<br />

## **2.4 The Performance of GPT on Automated Program Repair**

In the field of Automated Program Repair (APR), GPT Shows results that surpass existing state-of-the-art APR technologies. However, there is a data leakage issues where datasets commonly used in APR tasks are included in the training data, which may lead to overestimation of the performance of LLM. There is a possibility that the performance of LLM may be overestimated due to data leakage when performing bug fixing tasks in a process trained with massive data obtained from the Internet.

 <br />

Quanjun Zhang et al. [10] collected competition programming problems and user-submitted codes that did not cause data leaks, built a dataset as follows, and conducted an APR performance evaluation of ChatGPT.

 <br />

l **Raw data collection:** Crawling exercises and all Java submitted code

l **Obtain fix diffs of buggy code:** keep only pairs with differences of less than 6 tokens between the buggy program and the correct program.

l **Test case extraction:** Download more test cases from the competition’s dedicated database

l **Static-based filtering:** Remove repetitive submissions and comments within code to ensure not to affect GPT judgment

l **Dynamic-based filtering:** Run all remaining submissions and remove pairs if they fail the test cases

 <br />

As a result of the experiment, three results were confirmed in the APR ability evaluation of ChatGPT. First, ChatGPT was able to fix 109 out of 151 bugs when given the default prompt. Second, Chat GPT was able to fix 18, 25, and 10 additional bugs by adding respectively a programming problem description, error message, and bug location to the prompt. Third, ChatGPT was able to fix 9 bugs that were not fixed by default prompts or prompts containing error information.

<br />

Based on the above experimental results, it is expected that GPT’s outstanding code modification ability in the APR field can also be utilized in generating code feedback to support programming language learning. Furthermore, it was interesting to discover the impact of the comments in the submitted code on the performance of LLM during the data collection and preprocessing process.

<br />

 When designing a code review module using GPT, a step to remove comments in code submitted by learners must be included. Additionally, adding a problem description, error message, and bug location to the prompt has proven effective in improving bug fixing performance.

<br />

## **2.5 LLMs in Computer Science Class**

The recent LLMs, such as the recently released ChatGPT and GitHub Copilot, are receiving significant attention from computer science education experts. Education experts have found that LLMs can generate accurate solutions and accurately explain code for a variety of introductory programming assignments. The outstanding capabilities of these LLMs are generating ongoing discussions about how programming should be taught in computer science classes. At this point, the opinions of computer science instructors are divided. Lau and Guo [11] interviewed 20 introductory programming instructors about how to apply it to their classes. The author responded that in the short term, many instructors will ban the use of LLMs in class to prevent AI-assist cheating. On the other hand, the remaining respondents responded that they would be willing to introduce LLMs into their classes.

<br />

Through prior research, LLMs have confirmed the possibility of creating learner-friendly feedback by designing prompts. Therefore, in order to meaningfully utilize LLMs in computer science classes, fit-for-purpose prompt engineering is necessary. In particular, in the introductory programming class, where students learn programming language proficiency and algorithms by solving various programming problems, the rules of using ChatGPT, which includes Codex, a powerful code generation model, are needed. If novice learners use ChatGPT to easily generate code through prompts to solve problems, their learning effectiveness may decrease and they may become dependent on GPT.

<br />

To prevent AI-assist cheating side effects that may occur when GPT is introduced into programming language classes, prompts and system flows for generating code review comments need to be designed effectively and systematically. For prompts, Majeed Kazemitabaar [5] suggests including the following principles when designing prompts:

l Include at least one input/output example

l Structuring the response, including markdown and delimiters for display

l Verify the accuracy of submitted code and ensure technical accuracy necessary for code modification

l Use a style and tone that does not make students uncomfortable

<br />

When constructing the code feedback system, it is necessary to effectively display the response results generated by GPT through prompts on the web-front by distinguishing between the function of annotating code lines that require modification based on Markdown and learner-friendly feedback.

<br />

# **Chapter3. Method**

This study was conducted through the following procedures: dataset collection, prompt template design and enhancement, design guideline establishment, initial system design and development, usability test and improvement, and improved system evaluation. These procedures can be presented in Figure 2.

<br />

![fig2](../../images/2024-06-27-prj7/fig2.png)

Figure 2 Research Methodology

<br />

 In the dataset collection stage, test data for testing prompt template was obtained using logs of an existing online judge system. In the prompt template design and enhancement stage, the code review module was designed to develop prompt templates, and then the module was advanced by testing with collected dataset. In the system design guideline establishment stage, improvements were identified and a system design guideline was derived through literature research and comparison with the judge system. In the initial system development stage, the development settings for deploying service was established, the functions of modules were designed and implemented, and UI was built. In the Usability Test and Improvement stage, a usability test was conducted on the initial system and improvement strategy was established based on the results.

<br />

 Finally, after improving the initial system, the degree of performance of the improved system was evaluated according to research queries, and its suitability as a programming language learning support tool for primary and secondary school students was verified.

<br />

## **3.1 Dataset Collection**

In order to systematically manage test data, the data frame was composed of eight-labels as follows:

<br />

l **Exercise ID (Ex.ID):** Primary key of Exercise registered in the online judge system

l **Title:** Name of exercise

l **Description (Desc):** An instruction containing the exercise requirements, input examples, and output examples

l **Solution:** Instructor’s answer code

l **Sub. Code (Submitted Code from students):** Code submitted by students

l **Solved Subs (The number of Solved Submitted Code):** The number of correct answers among the codes submitted by students

l **Total Subs (The total number of Submitted Code):** The total number of codes submitted by students

l **Accuracy:** Proportion rate of correct answers

<br />

 Exercises, students’ submitted codes, and instructors’ answer codes were collected from Company C’s Online Judge System [12] for the introductory Python programming course obtained by referencing the data collection method in the 2023 AtCoder Competition [10] used by Quanjun Zhang et al.

<br />

l **Raw data collection:** Starting in 2021 year, among the codes submitted in Python in Company C’s Online Judge System, exercise, student’s submitted code, and the solution were secured

l **Static-based filtering:** Remove repeatedly submitted code and delete comments within the code to ensure they do not affect GPT performance

<br />

 A total of 93 test data for 27 questions that underwent static-based filtering were entered into the data frame, and an example can be found in Table 1.

![t1](../../images/2024-06-27-prj7/t1.png)

Table 1 Example of Test Data on Data Frame

<br />

## **3.2 Prompt Template Design and Enhancement**

 To Provide code feedback, which is the core function of this system, a code review module was designed, based on the three-stage feedback generation pipeline proposed by J. Lu et al [9]. In order to provide learner-friendly feedback suitable for primary and secondary school students, comments in code reviews are technically accurate. In addition, to be easily distinguished and exposed in the comment display area and code editor area, feedback generated through must be created in markdown format. Furthermore, the style and tone of code review comments must be supportive and constructive, taking into account the target age. The code review module consists of three main prompts (Role setting, Review Necessity Prediction, and Review Comment Generation).

 

l **Role-setting prompt:** Processes learner’s requests by setting the role of the model

l **Review Necessity Prediction prompt:** Responds with a binary label of “yes” or “no” indicating the need for review of the code submitted by the learner. If “no”, end response

l **Review Comment Generation prompt:** Generates comments for code reviews

n **Style & Tone:** Indicates the tone or style of the response

n **Instruction:** Requirements for adding markdown so that code review results can be distinguished and displayed on the web front

n **Restriction:** Instruction not to present directly solution code

n **Exercise:** Description of practice problem selected by the learner in the online judge system

n **Submitted Code:** Code submitted by the learner

n **Solution:** Answer code written by the instructor

n **Example:** Ideal prompt example for RCGP

 

 In the process of upgrading the prompt, three main criteria were derived. First, feedback must provide clear and useful value to users. Second, the module structure must be logical and intuitive. Third, modules must be optimized for efficient length while maintaining performance. Based on this standard, the code review model was updated a total of five times, and the following improvements were made during the update process: modifying markdown tags to fit the code review concept, minimizing input-token by changing markdown tags from Korean to English**,** changing the prompt from conversational type to noun type, improving the structure and order of prompts to ensure a logical and natural flow**.** 

 

 Through these improvements, the prompt length was shortened from 722 characters including spaces (553 characters excluding spaces) to 701 characters (588 characters excluding spaces) in Korean. In particular, by adding solution and example, which are sub-prompts of RCGP, the hallucination phenomenon of comments was significantly reduced, and it was confirmed that the style and tone of comments were created in a consistent manner. 

